{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_tf_freecodecamp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7q0h6l71p6Uw8xNberM4V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakib06/AI/blob/master/nlp_tf_freecodecamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dANLbGUVjMCp"
      },
      "source": [
        "##  Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D5E9eSEvYXE"
      },
      "source": [
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-ECKcDiv6v0"
      },
      "source": [
        "def get_batch_data():\n",
        "  \n",
        "  (train_data, test_data), info = tfds.load('imdb_reviews/subwords8k',\n",
        "                                          split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "                                          with_info=True, as_supervised=True)\n",
        "  encoder = info.features['text'].encoder \n",
        "  padded_shapes = ([None], ())\n",
        "  train_batches = train_data.shuffle(1000).padded_batch(10,\n",
        "                                                        padded_shapes=padded_shapes)\n",
        "  test_batches = test_data.shuffle(1000).padded_batch(10, \n",
        "                                                      padded_shapes=padded_shapes)\n",
        "  return train_batches, test_batches, encoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quRdPJbcxYNw"
      },
      "source": [
        "def get_model(encoder, embedding_dim=16):\n",
        "  embedding_dim =16\n",
        "  model = keras.Sequential([layers.Embedding(encoder.vocab_size,embedding_dim),\n",
        "                          layers.GlobalAveragePooling1D(),\n",
        "                          layers.Dense(1, activation='sigmoid')\n",
        "                          ])\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQm8Qc98ZQQk"
      },
      "source": [
        "def plot_history(history):\n",
        "  history_dict = history.history\n",
        "  acc = history_dict['accuracy']\n",
        "  val_acc = history_dict['val_accuracy']\n",
        "  epochs = range(1, len(acc)+1)\n",
        "  print(type(epochs),epochs)\n",
        "  print(acc)\n",
        "  plt.figure(figsize=(12, 9))\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.ylim((0.5, 1))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99NcfmnSZeZY"
      },
      "source": [
        "\n",
        "def retrieve_embeddings(model, encoder):\n",
        "  out_vectors = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "  out_metadata = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "  weights = model.layers[0].get_weights()[0] # at 0 -> embeddig layer\n",
        "  for num,word in enumerate(encoder.subwords):\n",
        "    vec = weights[num+1]\n",
        "    out_metadata.write(word + '\\n')\n",
        "    out_metadata.write('\\t'.join([str(x) for x in vec])+'\\n')\n",
        "  out_vectors.close()\n",
        "  out_metadata.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7E95c6N7VDz"
      },
      "source": [
        "train_batches, test_batches, encoder = get_batch_data()\n",
        "model = get_model(encoder)\n",
        "history = model.fit(train_batches, epochs=10, validation_data=test_batches,\n",
        "                    validation_steps=20)\n",
        "plot_history(history)\n",
        "\n",
        "retrieve_embeddings(model, encoder)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty7X_JyojcUu"
      },
      "source": [
        "## Sentiment Analysis on Movie Reviews \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju6W232XZB8Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAGy-7aeZCeg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEiKGJ5T4suB"
      },
      "source": [
        "### Letâ€™s Write An AI That Writes Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE30FxEE4w2M"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "path_to_file = utils.get_file('shakespeare.txt',\n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file,'rb' ).read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4HgkV247M_S",
        "outputId": "be579e85-5f64-48fe-e510-a7bd0f642861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz'\n",
        "dataset = utils.get_file(\n",
        "    'stack_overflow_16k.tar.gz',\n",
        "    data_url,\n",
        "    untar=True,\n",
        "    cache_dir='stack_overflow',\n",
        "    cache_subdir='')\n",
        "dataset_dir = pathlib.Path(dataset).parent"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/stack_overflow_16k.tar.gz\n",
            "6053888/6053168 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-4af5f5195501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stack_overflow'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     cache_subdir='')\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdataset_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pathlib' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cikBuDSj7c2_"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}