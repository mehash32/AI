{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "cnn_sentence_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakib06/AI/blob/master/cnn_sentence_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "33f5679b5e95039a32bd91c4d9fe6e55c3e17462",
        "id": "dfD6u68E9p-1"
      },
      "source": [
        "**INTRODUCTION**\n",
        "\n",
        "Hi all, this kernel is an intorduction to text classification using deep leanring. It took some time for the deep learning approaches to make a mark on textual data but since then the impact of deep learning on NLP has had a vertical graph. \n",
        "\n",
        "In this kernel we will get our hands dirty with a well in demand problem of text/document classification, around 2014 yoon kim et al. started to experiment with the relevance of CNN in the field of NLP and since then there has been no looking back. In the paper \"[Convolutional Neural Networks for Sentence Classification](http://arxiv.org/pdf/1408.5882.pdf)\" yoon kim et al. experiments with multiple CNN models (single channel, multiple channel) on top of word embeddings for text classification.\n",
        "\n",
        "For the sake of simplicity we will start off with a single channel model with pretrasined Glove embeddings. The data set used is the famous [20_newsgroup dataset](http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/news20.html)(original dataset link).\n",
        "\n",
        "In this kernel we will first learn about the processing of dataset, followed by a keras implementation of text classification using the preexisting Glove embeddings. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a80121ade4ecf6c4caa264a5fd5cd5981778119a",
        "id": "_FXxyMbo9p_B"
      },
      "source": [
        "**THE APPROACH**\n",
        "\n",
        "The idea presented follows a flow like : \n",
        "<a href=\"https://imgur.com/xLrP6IM\"><img src=\"https://i.imgur.com/xLrP6IM.png\" title=\"source: imgur.com\" style=\"width:400px;height:600px;\"/></a>\n",
        "\n",
        "\n",
        "We basically add different convolution layers of filter sizes [3, 4, 5], this somewhat emulates different skip-gram models where different filter sizes essentially means the number of words the filter is being applied to. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0750388a2837b4425e5f1dd11da60a8f9b30c7b4",
        "id": "Ctn3WVKj9p_C"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
        "from keras.layers import MaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "daf453aefbb50f26e525042203cfcbf4b3976c57",
        "id": "zTZtvPAm9p_L"
      },
      "source": [
        "# just to make sure the dataset is added properly \n",
        "!ls '../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrZOxlPr-yaD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "35297be0266ec89dc1786312025a26458be584d6",
        "id": "6BvFdxfw9p_R"
      },
      "source": [
        "# the dataset path\n",
        "TEXT_DATA_DIR = r'../input/20-newsgroup-original/20_newsgroup/20_newsgroup/'\n",
        "#the path for Glove embeddings\n",
        "GLOVE_DIR = r'../input/glove6b/'\n",
        "# make the max word length to be constant\n",
        "MAX_WORDS = 10000\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "# the percentage of train test split to be applied\n",
        "VALIDATION_SPLIT = 0.20\n",
        "# the dimension of vectors to be used\n",
        "EMBEDDING_DIM = 100\n",
        "# filter sizes of the different conv layers \n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 512\n",
        "embedding_dim = 100\n",
        "# dropout probability\n",
        "drop = 0.5\n",
        "batch_size = 30\n",
        "epochs = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "fa68305c64bc844cd86e776b3cb37e3661f6f652",
        "id": "KHkcMJYM9p_W"
      },
      "source": [
        "**DATASET STRUCTURE**\n",
        "\n",
        "The dataset is present in a hierarchical structure, i.e. all files of a given class are located in their respective folders and each datapoint has its own '.txt' file.\n",
        "\n",
        "* First we go through the entire dataset to build our text list and label list. \n",
        "* Followed by this we tokenize the entire data using Tokenizer, which is a part of keras.preprocessing.text.\n",
        "* We then add padding to the sequences to make them of a uniform length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8f9972b8b95a38df4e08227b5a638bd675d7c945",
        "id": "Byez_SwT9p_X"
      },
      "source": [
        "## preparing dataset\n",
        "\n",
        "\n",
        "texts = []  # list of text samples\n",
        "labels_index = {}  # dictionary mapping label name to numeric id\n",
        "labels = []  # list of label ids\n",
        "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
        "    path = os.path.join(TEXT_DATA_DIR, name)\n",
        "    if os.path.isdir(path):\n",
        "        label_id = len(labels_index)\n",
        "        labels_index[name] = label_id\n",
        "        for fname in sorted(os.listdir(path)):\n",
        "            if fname.isdigit():\n",
        "                fpath = os.path.join(path, fname)\n",
        "                if sys.version_info < (3,):\n",
        "                    f = open(fpath)\n",
        "                else:\n",
        "                    f = open(fpath, encoding='latin-1')\n",
        "                t = f.read()\n",
        "                i = t.find('\\n\\n')  # skip header\n",
        "                if 0 < i:\n",
        "                    t = t[i:]\n",
        "                texts.append(t)\n",
        "                f.close()\n",
        "                labels.append(label_id)\n",
        "print(labels_index)\n",
        "\n",
        "print('Found %s texts.' % len(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f3058f0c6703374a384d8720712cb2151e44e8ca",
        "id": "KgoI607C9p_b"
      },
      "source": [
        "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences =  tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"unique words : {}\".format(len(word_index)))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fc1c709458e9f4eb338a40c40c85dedba29c6fe8",
        "id": "Uv14NesE9p_e"
      },
      "source": [
        "# split the data into a training set and a validation set\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
        "\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "2ba8039ec130a51f64bad77c718a7f2e91e13d19",
        "id": "BGB2Vwqd9p_h"
      },
      "source": [
        "Since we have our train-validation split ready, our next step is to create an embedding matrix from the precomputed Glove embeddings.\n",
        "For convenience we are freezing the embedding layer i.e we will not be fine tuning the word embeddings. Feel free to test it out for better accuracy on very specific examples. From what can be seen, the Glove embeddings are universal features and tend to perform great in general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0620c11d2dab62329f250ecad40bcefbf57a7134",
        "id": "4sNlzgub9p_h"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "07d064695cf65aaba497d6bb0dbd14dea220d533",
        "id": "t2_URTlp9p_m"
      },
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "74abe6ec0048d25c6169081f7cd409359588aee0",
        "id": "HpNeeyuQ9p_q"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e499119a397f180258ab0e2b8c5a6b47ef98fc7c",
        "id": "tvzyCCx79p_s"
      },
      "source": [
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedding = embedding_layer(inputs)\n",
        "\n",
        "print(embedding.shape)\n",
        "reshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(embedding)\n",
        "print(reshape.shape)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
        "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
        "maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
        "\n",
        "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=20, activation='softmax')(dropout)\n",
        "\n",
        "# this creates a model that includes\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
        "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c3f99fb84e45c5fdf63607020c346902c340a31a",
        "scrolled": true,
        "id": "AZK0BxoO9p_w"
      },
      "source": [
        "print(\"Traning Model...\")\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(x_val, y_val))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "6dd11fdc4f1d2898adb4103d7794a7d4a3919d4c",
        "id": "57pIf9749p_y"
      },
      "source": [
        "I hope this Kernel was helpful for you, any sort of feedback and comments are appreciated. Feel free to reach out in case something is unclear.\n",
        "\n",
        "Until next time, Happy learning :) . . .. ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "_uuid": "4da2bc289e1d27d5225f68cb33352347e737c8a4",
        "id": "mGifW5LN9p_1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}